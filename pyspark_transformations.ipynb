{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbUsKRRChNb2vqLxV6o++e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumitsaxena-git/databricks/blob/main/pyspark_transformations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0cC0mTuAtli",
        "outputId": "61f85be0-36c7-461a-d080-a6d632bffaf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ab005d7d2de3e1f0864a0629c5bde12245025603b528be39d56f4f69c1285f0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Colab\").getOrCreate()\n",
        "        # .master(\"local\")\\\n",
        "        # .config('spark.ui.port', '4050')\\\n",
        ""
      ],
      "metadata": {
        "id": "3S4lfv5WBDaC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext; in Spark Shell, this is already created for you and named 'sc'\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Create an RDD with 6 integers\n",
        "data = [4, 6, 3, 1, 5, 4]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Print the RDD\n",
        "print(rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W-METaUBDcn",
        "outputId": "97355822-e778-49c1-953f-75d485322f5b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 6, 3, 1, 5, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Partition in Spark\n",
        "# There are different ways to partition an RDD, effective way is using key-value pairs.\n",
        "# Other 2 ways don't always need to partition an RDD with a key-value pair.\n",
        "\n",
        "# 1. repartition(numPartitions): This method can be used to increase or decrease the number of partitions in an RDD. It does a full shuffle of the data, so it can be expensive for large RDDs.\n",
        "\n",
        "rdd = rdd.repartition(2)  # Repartition RDD into 2 partitions\n",
        "\n",
        "# 2. coalesce(numPartitions): This method is used to reduce the number of partitions in an RDD without a full shuffle, by merging partitions.\n",
        "\n",
        "rdd = rdd.coalesce(2)  # Coalesce RDD into 2 partitions\n",
        "\n",
        "# 3. Using Transformations: You can also use other transformations like mapPartitions, flatMap, etc., to create new RDDs with custom partitioning logic.\n",
        "\n",
        "rdd = rdd.mapPartitions(lambda partition: ...)  # Define custom partitioning log"
      ],
      "metadata": {
        "id": "VBG5B1tQGsXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map Transformation\n",
        "rdd_map = rdd.map(lambda x:x*x)\n",
        "print(rdd_map.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0-DFOcyBDfW",
        "outputId": "748a6225-40f4-4de8-b5a9-b4d88edfe24b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16, 36, 9, 1, 25, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mapPartition Transformation\n",
        "\n",
        "# Create RDD with 2 partitions\n",
        "rdd_partitioned = sc.parallelize(data, 2)\n",
        "\n",
        "# Apply mapPartitions transformation\n",
        "rdd_map_partitions = rdd_partitioned.mapPartitions(lambda partition: map(lambda x: x*x, partition))\n",
        "\n",
        "# Print the partitioned RDD, glom() method is used to collect all elements of each partition into a list for easier viewing\n",
        "print(rdd_map_partitions.glom().collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUIeViz-Dcw0",
        "outputId": "6a12886e-4e89-4866-c7ab-5c073415b913"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[16, 36, 9], [1, 25, 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the RDD\n",
        "rdd_sorted_asc  = rdd.sortBy(lambda x:x)\n",
        "\n",
        "# Sort the RDD in descending order\n",
        "rdd_sorted_desc = rdd.sortBy(lambda x: x, ascending=False)\n",
        "\n",
        "print(rdd_sorted_asc.collect())\n",
        "print(rdd_sorted_desc.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W83FJ_jcDczb",
        "outputId": "4c4d08bb-232c-4fc7-d098-d3ebc10e8ee8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 4, 4, 5, 6]\n",
            "[6, 5, 4, 4, 3, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distinct RDD\n",
        "rdd_dist = rdd.distinct()\n",
        "print(rdd_dist.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFXcOaltDc3C",
        "outputId": "fcb038b9-a3a8-45cc-eb97-385ec316fd86"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 6, 3, 1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtered RDD\n",
        "rdd_filtered = rdd.filter(lambda x:x%2==0)\n",
        "print(rdd_filtered.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNCwzOKXDc9C",
        "outputId": "960e166f-d133-4e9b-88eb-ae631ab4de37"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 6, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reduceByKey\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext; in Spark Shell, this is already created for you and named 'sc'\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Create an RDD with key-value pairs\n",
        "data = [(\"a\", 4), (\"b\", 6), (\"a\", 3), (\"c\", 1), (\"b\", 5), (\"c\", 4)]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Example using reduceByKey\n",
        "rdd_reduce_by_key = rdd.reduceByKey(lambda x, y: x + y)\n",
        "print(\"reduceByKey result:\")\n",
        "print(rdd_reduce_by_key.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jISl9YJZOD4B",
        "outputId": "3cb67d54-aaf3-45c5-c6ee-7cb926d621e3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reduceByKey result:\n",
            "[('b', 11), ('c', 5), ('a', 7)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DgJ_QkXtBDi4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}